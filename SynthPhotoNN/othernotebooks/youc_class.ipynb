{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data saved\n",
      "\n",
      "condition on dg_fpsf cut 1 inputs\n",
      "condition on dr_fpsf cut 0 inputs\n",
      "condition on di_fpsf cut 0 inputs\n",
      "condition on dz_fpsf cut 0 inputs\n",
      "condition on dy_fpsf cut 0 inputs\n",
      "condition on dCaHK_0 cut 0 inputs\n",
      "condition on du_0 cut 0 inputs\n",
      "condition on dFeHadop cut 17 inputs\n",
      "3.13% of inputs cut\n",
      "Num stars in class -2.0: 2760\n",
      "Num stars in class -2.5: 2760\n",
      "Num stars in class -3.0: 2285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "import sys\n",
    "sys.path.append(\"/home/watkinsz/Desktop/For_Zack/ML\")\n",
    "sys.path.append(\"/home/watkinsz/Desktop/For_Zack/ML/FeH\")\n",
    "from fitsParser.fitsParser import fitsParser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from numpy import linspace\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from modelValidator import psMetric, cut, shuffleDwarfs, plotRes, plot, norm_inputs, monte_carlo, printTable, conf_matrix\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as gpr\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from function import save_data\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import Nadam\n",
    "binsize = 0.5\n",
    "\n",
    "def getMags(colour):\n",
    "    return colour[:colour.index('-')], colour[colour.index('-')+1:]\n",
    "sdss = fitsParser(includefile = 'FeHdata.txt')\n",
    "youc = sdss.getAllObj()[1]\n",
    "cut(youc, metThresh = -5)\n",
    "youc.data = youc.data[youc.data['FeHadop']<-2.0]\n",
    "youc.makeColours(N=10)\n",
    "colours = ['u-g', 'g-i','g-r', 'u-i', 'u-r']\n",
    "conversion = {\n",
    "     'u_0' : 'u',\n",
    "    'CaHK_0' : 'CaHK' \n",
    "}\n",
    "\n",
    "\n",
    "conversion.update({x+'_fpsf' : x for x in 'grizy'})\n",
    "for col in youc.colours:\n",
    "    mag1, mag2 = getMags(col)\n",
    "    youc.data[conversion[mag1]+'-'+conversion[mag2]] = youc.data[col]\n",
    "\n",
    "for field in youc.data.keys():\n",
    "    if field in conversion:\n",
    "        youc.data[conversion[field]]= youc.data[field]\n",
    "youc.data = youc.data.sample(frac=1)\n",
    "\n",
    "youc.data = youc.data[youc.data['FeHadop']<-2]\n",
    "youc.data['test_flag'] = [0]*int(0.8*len(youc.data))+[1]*(len(youc.data)-int(0.8*len(youc.data)))\n",
    "\n",
    "classes = [-2 - binsize*i for i in range(int((3-2)/binsize)+1)]     \n",
    "\n",
    "putbin = lambda data : [x + min([c-x for c in classes if c>x]) if x<-2 else -2 for x in data]\n",
    "youc.data['FeHround'] = putbin(youc.data['FeHadop'])\n",
    "\n",
    "def baseModel(inputsDim, outDim, Nlayers=30, Nodes = 128):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(Nodes, input_dim = inputsDim, activation = 'relu'))\n",
    "    \n",
    "    for i in range(Nlayers-1):\n",
    "        model.add(Dense(Nodes, activation = 'relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(outDim, activation = 'softmax'))\n",
    "    op = Nadam(lr = 0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=op, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def monte(din, N=3):\n",
    "    tous = pd.DataFrame(din)\n",
    "    uncert = pd.DataFrame()\n",
    "    for col in youc.colours:\n",
    "        c1 = 'd'+re.compile('.+(?=-)').findall(col)[0]\n",
    "        c2 = 'd' + re.compile('(?<=-).+').findall(col)[0]\n",
    "        uncert[col] = np.sqrt(tous[c1]**2+tous[c2]**2)\n",
    "\n",
    "    uncert[map(lambda x : x[1:], youc.output_uncert)] = tous[youc.output_uncert]\n",
    "    return monte_carlo(tous[list(youc.colours+youc.outputs)],uncert ,N=N)\n",
    "\n",
    "montestyle = 2\n",
    "\n",
    "\n",
    "maxlen = len(youc.data[youc.data['FeHround']==classes[0]])\n",
    "minlen = min([len(youc.data[youc.data['FeHround']==c]) for c in classes])\n",
    "\n",
    "\n",
    "\n",
    "if montestyle == 1:\n",
    "    dprelim = youc.data[(youc.data['FeHround']==classes[0])&(youc.data['test_flag']==0)]\n",
    "    train = dprelim\n",
    "    for c in classes[1:]:\n",
    "        datac = pd.DataFrame(youc.data[(youc.data['FeHround']==c)&(youc.data['test_flag']==0)])\n",
    "        l = len(datac)\n",
    "        print 'Num stars in class %.1f: %d'%(c,l)\n",
    "        if l!=0:\n",
    "            datac = monte(datac, N = maxlen/len(datac)-1)\n",
    "            train = train[datac.keys()]\n",
    "            train = train.append(datac, sort=True).reset_index(drop=True)\n",
    "if montestyle == 2:\n",
    "    nmonte = 60\n",
    "    dprelim  = youc.data[youc.data['test_flag']==0]\n",
    "    dprelim = monte(dprelim, N = nmonte )\n",
    "    train = pd.DataFrame()\n",
    "    dprelim['FeHround'] = putbin(dprelim['FeHadop'])\n",
    "    for c in classes:\n",
    "        toapp =  dprelim[dprelim['FeHround']==c]\n",
    "        train = train.append(toapp.sample(frac = nmonte*1.0*minlen/len(toapp) if nmonte*1.0*minlen/len(toapp)<1 else 1))\n",
    "\n",
    "\n",
    "train['FeHround'] = [-3 if x<=-3 else -2.5 if x<=-2.5 else -2 for x in train['FeHadop']]\n",
    "train = train.sample(frac = 1).reset_index(drop= True)\n",
    "for c in classes:\n",
    "        datac = pd.DataFrame(train[(train['FeHround']==c)])\n",
    "        l = len(datac)\n",
    "        print 'Num stars in class %.1f: %d'%(c,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_man:\n",
    "    \n",
    "    def __init__(self, classes, func, inverse_func):\n",
    "        self.classes = classes\n",
    "        self.func = func\n",
    "        self.inverse_func = inverse_func\n",
    "\n",
    "    def transform(self,data):\n",
    "        return [[1 if self.func(x, c) else 0 for c in self.classes] for x in data]\n",
    "    \n",
    "    def inverse_transform(self, trans_data):\n",
    "        return [self.inverse_func(x) for x in trans_data]\n",
    "def savemodel(model, params, data, filename):\n",
    "    joblib.dump(model, filename+'.P')\n",
    "    joblib.dump(params,  filename+\"Params.P\")\n",
    "    joblib.dump(data, filename + \"Data.P\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7805 samples, validate on 132 samples\n",
      "Epoch 1/200\n",
      " - 2s - loss: 1.0913 - acc: 0.3558 - val_loss: 1.0262 - val_acc: 0.6212\n",
      "Epoch 2/200\n",
      " - 1s - loss: 1.0893 - acc: 0.3507 - val_loss: 1.0293 - val_acc: 0.6439\n",
      "Epoch 3/200\n",
      " - 1s - loss: 1.0856 - acc: 0.3707 - val_loss: 0.9699 - val_acc: 0.6212\n",
      "Epoch 4/200\n",
      " - 0s - loss: 1.0826 - acc: 0.3687 - val_loss: 1.0623 - val_acc: 0.4015\n",
      "Epoch 5/200\n",
      " - 0s - loss: 1.0795 - acc: 0.3909 - val_loss: 0.9891 - val_acc: 0.5152\n",
      "Epoch 6/200\n",
      " - 0s - loss: 1.0857 - acc: 0.3748 - val_loss: 1.1064 - val_acc: 0.3485\n",
      "Epoch 7/200\n",
      " - 0s - loss: 1.0772 - acc: 0.3812 - val_loss: 1.0092 - val_acc: 0.4924\n",
      "Epoch 8/200\n",
      " - 1s - loss: 1.0729 - acc: 0.3996 - val_loss: 1.0436 - val_acc: 0.4621\n",
      "Epoch 9/200\n",
      " - 1s - loss: 1.0655 - acc: 0.4202 - val_loss: 0.9593 - val_acc: 0.4773\n",
      "Epoch 10/200\n",
      " - 0s - loss: 1.0970 - acc: 0.3791 - val_loss: 1.0737 - val_acc: 0.4091\n",
      "Epoch 11/200\n",
      " - 0s - loss: 1.0822 - acc: 0.3967 - val_loss: 1.0483 - val_acc: 0.4545\n",
      "Epoch 12/200\n",
      " - 0s - loss: 1.0669 - acc: 0.4115 - val_loss: 0.9493 - val_acc: 0.5530\n",
      "Epoch 13/200\n",
      " - 1s - loss: 1.0744 - acc: 0.3933 - val_loss: 1.0137 - val_acc: 0.5152\n",
      "Epoch 14/200\n",
      " - 1s - loss: 1.0655 - acc: 0.4163 - val_loss: 0.9828 - val_acc: 0.4848\n",
      "Epoch 15/200\n",
      " - 0s - loss: 1.0697 - acc: 0.4028 - val_loss: 0.9732 - val_acc: 0.5000\n",
      "Epoch 16/200\n",
      " - 0s - loss: 1.0595 - acc: 0.4128 - val_loss: 0.9105 - val_acc: 0.6136\n",
      "Epoch 17/200\n",
      " - 0s - loss: 1.0661 - acc: 0.4058 - val_loss: 1.2109 - val_acc: 0.2803\n",
      "Epoch 18/200\n",
      " - 0s - loss: 1.0650 - acc: 0.4155 - val_loss: 1.0284 - val_acc: 0.4848\n",
      "Epoch 19/200\n",
      " - 0s - loss: 1.0668 - acc: 0.4197 - val_loss: 1.0581 - val_acc: 0.4621\n",
      "Epoch 20/200\n",
      " - 0s - loss: 1.0647 - acc: 0.4108 - val_loss: 1.0797 - val_acc: 0.2955\n",
      "Epoch 21/200\n",
      " - 0s - loss: 1.0601 - acc: 0.4165 - val_loss: 1.0055 - val_acc: 0.4621\n",
      "Epoch 22/200\n",
      " - 0s - loss: 1.0563 - acc: 0.4205 - val_loss: 1.0183 - val_acc: 0.5379\n",
      "Epoch 23/200\n",
      " - 0s - loss: 1.0547 - acc: 0.4345 - val_loss: 1.0451 - val_acc: 0.4545\n",
      "Epoch 24/200\n",
      " - 0s - loss: 1.0736 - acc: 0.4067 - val_loss: 1.0707 - val_acc: 0.3106\n",
      "Epoch 25/200\n",
      " - 0s - loss: 1.0644 - acc: 0.4077 - val_loss: 0.9889 - val_acc: 0.4848\n",
      "Epoch 26/200\n",
      " - 0s - loss: 1.0581 - acc: 0.4104 - val_loss: 0.9889 - val_acc: 0.4773\n",
      "Epoch 27/200\n",
      " - 1s - loss: 1.0595 - acc: 0.4164 - val_loss: 1.1153 - val_acc: 0.4091\n",
      "Epoch 28/200\n",
      " - 0s - loss: 1.0602 - acc: 0.4142 - val_loss: 1.0541 - val_acc: 0.4848\n",
      "Epoch 29/200\n",
      " - 0s - loss: 1.0544 - acc: 0.4158 - val_loss: 1.0394 - val_acc: 0.4773\n",
      "Epoch 30/200\n",
      " - 0s - loss: 1.0503 - acc: 0.4283 - val_loss: 0.9655 - val_acc: 0.5303\n",
      "Epoch 31/200\n",
      " - 0s - loss: 1.0542 - acc: 0.4238 - val_loss: 0.9512 - val_acc: 0.5152\n",
      "Epoch 32/200\n",
      " - 0s - loss: 1.0691 - acc: 0.4059 - val_loss: 1.0257 - val_acc: 0.3030\n",
      "Epoch 33/200\n",
      " - 0s - loss: 1.0534 - acc: 0.4143 - val_loss: 0.9476 - val_acc: 0.5076\n",
      "Epoch 34/200\n",
      " - 0s - loss: 1.0538 - acc: 0.4114 - val_loss: 1.0379 - val_acc: 0.4621\n",
      "Epoch 35/200\n",
      " - 0s - loss: 1.0531 - acc: 0.4197 - val_loss: 0.9013 - val_acc: 0.6439\n",
      "Epoch 36/200\n",
      " - 0s - loss: 1.0737 - acc: 0.3951 - val_loss: 0.9821 - val_acc: 0.5530\n",
      "Epoch 37/200\n",
      " - 0s - loss: 1.0596 - acc: 0.4154 - val_loss: 1.0741 - val_acc: 0.4470\n",
      "Epoch 38/200\n",
      " - 0s - loss: 1.0625 - acc: 0.4070 - val_loss: 0.9603 - val_acc: 0.5000\n",
      "Epoch 39/200\n",
      " - 0s - loss: 1.0444 - acc: 0.4304 - val_loss: 1.1324 - val_acc: 0.4470\n",
      "Epoch 40/200\n",
      " - 0s - loss: 1.0527 - acc: 0.4343 - val_loss: 1.1103 - val_acc: 0.4091\n",
      "Epoch 41/200\n",
      " - 0s - loss: 1.0476 - acc: 0.4314 - val_loss: 0.9820 - val_acc: 0.5076\n",
      "Epoch 42/200\n",
      " - 0s - loss: 1.0501 - acc: 0.4286 - val_loss: 0.9521 - val_acc: 0.5530\n",
      "Epoch 43/200\n",
      " - 0s - loss: 1.0539 - acc: 0.4340 - val_loss: 1.1145 - val_acc: 0.2955\n",
      "Epoch 44/200\n",
      " - 0s - loss: 1.0577 - acc: 0.4020 - val_loss: 0.9742 - val_acc: 0.4848\n",
      "Epoch 45/200\n",
      " - 0s - loss: 1.0575 - acc: 0.4028 - val_loss: 0.9526 - val_acc: 0.5076\n",
      "Epoch 46/200\n",
      " - 0s - loss: 1.0528 - acc: 0.4094 - val_loss: 0.9460 - val_acc: 0.5000\n",
      "Epoch 47/200\n",
      " - 0s - loss: 1.0520 - acc: 0.4184 - val_loss: 0.9767 - val_acc: 0.4773\n",
      "Epoch 48/200\n",
      " - 0s - loss: 1.0449 - acc: 0.4232 - val_loss: 0.9592 - val_acc: 0.4924\n",
      "Epoch 49/200\n",
      " - 0s - loss: 1.0423 - acc: 0.4334 - val_loss: 1.0013 - val_acc: 0.4470\n",
      "Epoch 50/200\n",
      " - 0s - loss: 1.0455 - acc: 0.4297 - val_loss: 1.0007 - val_acc: 0.4394\n",
      "Epoch 51/200\n",
      " - 0s - loss: 1.0433 - acc: 0.4401 - val_loss: 0.8859 - val_acc: 0.5758\n",
      "Epoch 52/200\n",
      " - 0s - loss: 1.0541 - acc: 0.4315 - val_loss: 0.9418 - val_acc: 0.5152\n",
      "Epoch 53/200\n",
      " - 0s - loss: 1.0650 - acc: 0.4108 - val_loss: 1.0169 - val_acc: 0.3485\n",
      "Epoch 54/200\n",
      " - 0s - loss: 1.0574 - acc: 0.4181 - val_loss: 1.0721 - val_acc: 0.4621\n",
      "Epoch 55/200\n",
      " - 0s - loss: 1.0546 - acc: 0.4193 - val_loss: 1.0413 - val_acc: 0.4773\n",
      "Epoch 56/200\n",
      " - 0s - loss: 1.0562 - acc: 0.4141 - val_loss: 1.0218 - val_acc: 0.4394\n",
      "Epoch 57/200\n",
      " - 0s - loss: 1.0425 - acc: 0.4322 - val_loss: 1.0184 - val_acc: 0.5076\n",
      "Epoch 58/200\n",
      " - 0s - loss: 1.0474 - acc: 0.4384 - val_loss: 0.9180 - val_acc: 0.5152\n",
      "Epoch 59/200\n",
      " - 0s - loss: 1.0449 - acc: 0.4333 - val_loss: 0.9425 - val_acc: 0.5152\n",
      "Epoch 60/200\n",
      " - 0s - loss: 1.0512 - acc: 0.4259 - val_loss: 1.0494 - val_acc: 0.3258\n",
      "Epoch 61/200\n",
      " - 0s - loss: 1.0409 - acc: 0.4320 - val_loss: 0.9140 - val_acc: 0.5303\n",
      "Epoch 62/200\n",
      " - 0s - loss: 1.0540 - acc: 0.4169 - val_loss: 1.0215 - val_acc: 0.5152\n",
      "Epoch 63/200\n",
      " - 0s - loss: 1.0389 - acc: 0.4413 - val_loss: 0.9909 - val_acc: 0.5152\n",
      "Epoch 64/200\n",
      " - 0s - loss: 1.0404 - acc: 0.4446 - val_loss: 1.0819 - val_acc: 0.4924\n",
      "Epoch 65/200\n",
      " - 0s - loss: 1.0300 - acc: 0.4606 - val_loss: 0.8530 - val_acc: 0.5909\n",
      "Epoch 66/200\n",
      " - 0s - loss: 1.0423 - acc: 0.4479 - val_loss: 1.0666 - val_acc: 0.3182\n",
      "Epoch 67/200\n",
      " - 0s - loss: 1.0371 - acc: 0.4427 - val_loss: 0.9866 - val_acc: 0.4924\n",
      "Epoch 68/200\n",
      " - 0s - loss: 1.0341 - acc: 0.4514 - val_loss: 1.0250 - val_acc: 0.3712\n",
      "Epoch 69/200\n",
      " - 0s - loss: 1.0415 - acc: 0.4424 - val_loss: 1.0263 - val_acc: 0.3636\n",
      "Epoch 70/200\n",
      " - 0s - loss: 1.0475 - acc: 0.4181 - val_loss: 1.0098 - val_acc: 0.4394\n",
      "Epoch 71/200\n",
      " - 0s - loss: 1.0487 - acc: 0.4199 - val_loss: 1.0537 - val_acc: 0.4545\n",
      "Epoch 72/200\n",
      " - 0s - loss: 1.0458 - acc: 0.4261 - val_loss: 1.0046 - val_acc: 0.5758\n",
      "Epoch 73/200\n",
      " - 0s - loss: 1.0367 - acc: 0.4479 - val_loss: 0.9994 - val_acc: 0.6439\n",
      "Epoch 74/200\n",
      " - 0s - loss: 1.0364 - acc: 0.4487 - val_loss: 1.0459 - val_acc: 0.4394\n",
      "Epoch 75/200\n",
      " - 0s - loss: 1.0314 - acc: 0.4515 - val_loss: 0.8937 - val_acc: 0.6288\n",
      "Epoch 76/200\n",
      " - 0s - loss: 1.0463 - acc: 0.4409 - val_loss: 1.0129 - val_acc: 0.3864\n",
      "Epoch 77/200\n",
      " - 0s - loss: 1.0385 - acc: 0.4424 - val_loss: 1.0341 - val_acc: 0.4470\n",
      "Epoch 78/200\n",
      " - 0s - loss: 1.0302 - acc: 0.4570 - val_loss: 0.8359 - val_acc: 0.6515\n",
      "Epoch 79/200\n",
      " - 0s - loss: 1.0681 - acc: 0.4404 - val_loss: 0.9088 - val_acc: 0.5985\n",
      "Epoch 80/200\n",
      " - 0s - loss: 1.0275 - acc: 0.4620 - val_loss: 1.0535 - val_acc: 0.3712\n",
      "Epoch 81/200\n",
      " - 0s - loss: 1.0238 - acc: 0.4606 - val_loss: 1.1234 - val_acc: 0.3939\n",
      "Epoch 82/200\n",
      " - 0s - loss: 1.0224 - acc: 0.4687 - val_loss: 0.9457 - val_acc: 0.5606\n",
      "Epoch 83/200\n",
      " - 0s - loss: 1.0155 - acc: 0.4720 - val_loss: 0.8776 - val_acc: 0.6364\n",
      "Epoch 84/200\n",
      " - 0s - loss: 1.0495 - acc: 0.4495 - val_loss: 1.0379 - val_acc: 0.4318\n",
      "Epoch 85/200\n",
      " - 1s - loss: 1.0700 - acc: 0.4068 - val_loss: 1.0057 - val_acc: 0.4545\n",
      "Epoch 86/200\n",
      " - 0s - loss: 1.0577 - acc: 0.4179 - val_loss: 1.0083 - val_acc: 0.4242\n",
      "Epoch 87/200\n",
      " - 0s - loss: 1.0501 - acc: 0.4266 - val_loss: 0.9644 - val_acc: 0.4621\n",
      "Epoch 88/200\n",
      " - 0s - loss: 1.0420 - acc: 0.4405 - val_loss: 0.9711 - val_acc: 0.5379\n",
      "Epoch 89/200\n",
      " - 0s - loss: 1.0366 - acc: 0.4447 - val_loss: 1.1003 - val_acc: 0.4015\n",
      "Epoch 90/200\n",
      " - 0s - loss: 1.0415 - acc: 0.4437 - val_loss: 1.0984 - val_acc: 0.4697\n",
      "Epoch 91/200\n",
      " - 0s - loss: 1.0414 - acc: 0.4506 - val_loss: 1.0362 - val_acc: 0.4167\n",
      "Epoch 92/200\n",
      " - 0s - loss: 1.0371 - acc: 0.4504 - val_loss: 0.9357 - val_acc: 0.6136\n",
      "Epoch 93/200\n",
      " - 0s - loss: 1.0374 - acc: 0.4348 - val_loss: 1.0382 - val_acc: 0.3712\n",
      "Epoch 94/200\n",
      " - 0s - loss: 1.0323 - acc: 0.4428 - val_loss: 0.9032 - val_acc: 0.6591\n",
      "Epoch 95/200\n",
      " - 0s - loss: 1.0346 - acc: 0.4516 - val_loss: 1.0118 - val_acc: 0.4773\n",
      "Epoch 96/200\n",
      " - 0s - loss: 1.0367 - acc: 0.4454 - val_loss: 1.0233 - val_acc: 0.3864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200\n",
      " - 0s - loss: 1.0180 - acc: 0.4676 - val_loss: 0.8793 - val_acc: 0.6591\n",
      "Epoch 98/200\n",
      " - 0s - loss: 1.0261 - acc: 0.4564 - val_loss: 0.9635 - val_acc: 0.5076\n",
      "Epoch 99/200\n",
      " - 0s - loss: 1.0202 - acc: 0.4721 - val_loss: 0.8294 - val_acc: 0.6667\n",
      "Epoch 100/200\n",
      " - 0s - loss: 1.0290 - acc: 0.4502 - val_loss: 1.0720 - val_acc: 0.4242\n",
      "Epoch 101/200\n",
      " - 0s - loss: 1.0129 - acc: 0.4732 - val_loss: 0.9315 - val_acc: 0.5379\n",
      "Epoch 102/200\n",
      " - 0s - loss: 1.0180 - acc: 0.4666 - val_loss: 1.0367 - val_acc: 0.4242\n",
      "Epoch 103/200\n",
      " - 0s - loss: 1.0220 - acc: 0.4582 - val_loss: 0.8699 - val_acc: 0.6212\n",
      "Epoch 104/200\n",
      " - 0s - loss: 1.0238 - acc: 0.4620 - val_loss: 1.0568 - val_acc: 0.3788\n",
      "Epoch 105/200\n",
      " - 0s - loss: 1.0352 - acc: 0.4439 - val_loss: 0.9142 - val_acc: 0.6515\n",
      "Epoch 106/200\n",
      " - 0s - loss: 1.0222 - acc: 0.4673 - val_loss: 1.0982 - val_acc: 0.3864\n",
      "Epoch 107/200\n",
      " - 0s - loss: 1.0194 - acc: 0.4627 - val_loss: 0.9772 - val_acc: 0.5000\n",
      "Epoch 108/200\n",
      " - 0s - loss: 1.0157 - acc: 0.4719 - val_loss: 0.9897 - val_acc: 0.4621\n",
      "Epoch 109/200\n",
      " - 0s - loss: 1.0103 - acc: 0.4691 - val_loss: 0.9364 - val_acc: 0.5379\n",
      "Epoch 110/200\n",
      " - 0s - loss: 1.0122 - acc: 0.4752 - val_loss: 0.8636 - val_acc: 0.6212\n",
      "Epoch 111/200\n",
      " - 0s - loss: 1.0185 - acc: 0.4652 - val_loss: 1.0847 - val_acc: 0.4318\n",
      "Epoch 112/200\n",
      " - 0s - loss: 1.0106 - acc: 0.4670 - val_loss: 1.1752 - val_acc: 0.3712\n",
      "Epoch 113/200\n",
      " - 0s - loss: 1.0145 - acc: 0.4748 - val_loss: 0.9017 - val_acc: 0.5758\n",
      "Epoch 114/200\n",
      " - 1s - loss: 1.0273 - acc: 0.4542 - val_loss: 1.0900 - val_acc: 0.3333\n",
      "Epoch 115/200\n",
      " - 0s - loss: 1.0342 - acc: 0.4451 - val_loss: 0.9789 - val_acc: 0.5530\n",
      "Epoch 116/200\n",
      " - 1s - loss: 1.0125 - acc: 0.4721 - val_loss: 0.9030 - val_acc: 0.5833\n",
      "Epoch 117/200\n",
      " - 1s - loss: 1.0192 - acc: 0.4639 - val_loss: 0.8461 - val_acc: 0.6439\n",
      "Epoch 118/200\n",
      " - 1s - loss: 1.0139 - acc: 0.4729 - val_loss: 0.9989 - val_acc: 0.4545\n",
      "Epoch 119/200\n",
      " - 1s - loss: 1.0043 - acc: 0.4762 - val_loss: 1.0730 - val_acc: 0.4167\n",
      "Epoch 120/200\n",
      " - 0s - loss: 1.0084 - acc: 0.4693 - val_loss: 0.8828 - val_acc: 0.5985\n",
      "Epoch 121/200\n",
      " - 0s - loss: 1.0216 - acc: 0.4616 - val_loss: 1.0529 - val_acc: 0.3939\n",
      "Epoch 122/200\n",
      " - 0s - loss: 1.0105 - acc: 0.4765 - val_loss: 0.9416 - val_acc: 0.5758\n",
      "Epoch 123/200\n",
      " - 1s - loss: 1.0051 - acc: 0.4743 - val_loss: 0.8403 - val_acc: 0.6439\n",
      "Epoch 124/200\n",
      " - 1s - loss: 1.0222 - acc: 0.4615 - val_loss: 0.9464 - val_acc: 0.5909\n",
      "Epoch 125/200\n",
      " - 0s - loss: 1.0036 - acc: 0.4823 - val_loss: 0.9377 - val_acc: 0.5530\n",
      "Epoch 126/200\n",
      " - 1s - loss: 1.0120 - acc: 0.4758 - val_loss: 1.0072 - val_acc: 0.4318\n",
      "Epoch 127/200\n",
      " - 0s - loss: 1.0119 - acc: 0.4742 - val_loss: 0.9092 - val_acc: 0.6667\n",
      "Epoch 128/200\n",
      " - 0s - loss: 0.9987 - acc: 0.4856 - val_loss: 1.0468 - val_acc: 0.4773\n",
      "Epoch 129/200\n",
      " - 0s - loss: 1.0055 - acc: 0.4750 - val_loss: 0.9895 - val_acc: 0.5152\n",
      "Epoch 130/200\n",
      " - 0s - loss: 1.0206 - acc: 0.4661 - val_loss: 0.9932 - val_acc: 0.4773\n",
      "Epoch 131/200\n",
      " - 0s - loss: 1.0016 - acc: 0.4784 - val_loss: 0.8574 - val_acc: 0.6364\n",
      "Epoch 132/200\n",
      " - 0s - loss: 1.0029 - acc: 0.4758 - val_loss: 0.9203 - val_acc: 0.5455\n",
      "Epoch 133/200\n",
      " - 0s - loss: 1.0047 - acc: 0.4802 - val_loss: 0.9544 - val_acc: 0.5227\n",
      "Epoch 134/200\n",
      " - 0s - loss: 1.0047 - acc: 0.4748 - val_loss: 0.9129 - val_acc: 0.5682\n",
      "Epoch 135/200\n",
      " - 0s - loss: 0.9941 - acc: 0.4916 - val_loss: 0.9421 - val_acc: 0.5455\n",
      "Epoch 136/200\n",
      " - 1s - loss: 1.0185 - acc: 0.4689 - val_loss: 0.8871 - val_acc: 0.5833\n",
      "Epoch 137/200\n",
      " - 0s - loss: 0.9964 - acc: 0.4898 - val_loss: 0.9886 - val_acc: 0.4848\n",
      "Epoch 138/200\n",
      " - 0s - loss: 1.0046 - acc: 0.4743 - val_loss: 0.8418 - val_acc: 0.6288\n",
      "Epoch 139/200\n",
      " - 0s - loss: 1.0036 - acc: 0.4769 - val_loss: 0.9085 - val_acc: 0.5985\n",
      "Epoch 140/200\n",
      " - 0s - loss: 0.9988 - acc: 0.4824 - val_loss: 1.0274 - val_acc: 0.4773\n",
      "Epoch 141/200\n",
      " - 0s - loss: 0.9992 - acc: 0.4803 - val_loss: 0.9370 - val_acc: 0.5227\n",
      "Epoch 142/200\n",
      " - 0s - loss: 1.0116 - acc: 0.4739 - val_loss: 1.0764 - val_acc: 0.3712\n",
      "Epoch 143/200\n",
      " - 0s - loss: 1.0376 - acc: 0.4479 - val_loss: 0.9945 - val_acc: 0.4773\n",
      "Epoch 144/200\n",
      " - 1s - loss: 1.0053 - acc: 0.4816 - val_loss: 0.9320 - val_acc: 0.5909\n",
      "Epoch 145/200\n",
      " - 0s - loss: 1.0015 - acc: 0.4799 - val_loss: 1.1549 - val_acc: 0.4167\n",
      "Epoch 146/200\n",
      " - 0s - loss: 1.0067 - acc: 0.4717 - val_loss: 0.8581 - val_acc: 0.6515\n",
      "Epoch 147/200\n",
      " - 0s - loss: 0.9996 - acc: 0.4824 - val_loss: 0.9202 - val_acc: 0.5833\n",
      "Epoch 148/200\n",
      " - 0s - loss: 1.0006 - acc: 0.4834 - val_loss: 1.1232 - val_acc: 0.3939\n",
      "Epoch 149/200\n",
      " - 0s - loss: 1.0073 - acc: 0.4807 - val_loss: 0.9324 - val_acc: 0.5833\n",
      "Epoch 150/200\n",
      " - 0s - loss: 0.9959 - acc: 0.4823 - val_loss: 1.0605 - val_acc: 0.4621\n",
      "Epoch 151/200\n",
      " - 0s - loss: 1.0095 - acc: 0.4703 - val_loss: 0.8731 - val_acc: 0.6364\n",
      "Epoch 152/200\n",
      " - 0s - loss: 0.9934 - acc: 0.4879 - val_loss: 0.8780 - val_acc: 0.6288\n",
      "Epoch 153/200\n",
      " - 0s - loss: 1.0088 - acc: 0.4753 - val_loss: 0.8952 - val_acc: 0.5682\n",
      "Epoch 154/200\n",
      " - 0s - loss: 0.9970 - acc: 0.4894 - val_loss: 0.9260 - val_acc: 0.5606\n",
      "Epoch 155/200\n",
      " - 0s - loss: 0.9987 - acc: 0.4873 - val_loss: 0.8348 - val_acc: 0.6439\n",
      "Epoch 156/200\n",
      " - 0s - loss: 1.0103 - acc: 0.4684 - val_loss: 0.9253 - val_acc: 0.5682\n",
      "Epoch 157/200\n",
      " - 0s - loss: 0.9885 - acc: 0.4878 - val_loss: 0.8991 - val_acc: 0.6061\n",
      "Epoch 158/200\n",
      " - 1s - loss: 1.0042 - acc: 0.4788 - val_loss: 0.8402 - val_acc: 0.6364\n",
      "Epoch 159/200\n",
      " - 0s - loss: 0.9943 - acc: 0.4840 - val_loss: 0.9431 - val_acc: 0.5152\n",
      "Epoch 160/200\n",
      " - 0s - loss: 1.0045 - acc: 0.4826 - val_loss: 0.8139 - val_acc: 0.6439\n",
      "Epoch 161/200\n",
      " - 0s - loss: 1.0081 - acc: 0.4716 - val_loss: 0.8950 - val_acc: 0.5833\n",
      "Epoch 162/200\n",
      " - 0s - loss: 0.9925 - acc: 0.4892 - val_loss: 0.9292 - val_acc: 0.5530\n",
      "Epoch 163/200\n",
      " - 0s - loss: 0.9908 - acc: 0.4883 - val_loss: 0.8920 - val_acc: 0.5985\n",
      "Epoch 164/200\n",
      " - 0s - loss: 1.0059 - acc: 0.4770 - val_loss: 0.8556 - val_acc: 0.6136\n",
      "Epoch 165/200\n",
      " - 0s - loss: 0.9929 - acc: 0.4846 - val_loss: 0.8495 - val_acc: 0.6515\n",
      "Epoch 166/200\n",
      " - 0s - loss: 0.9903 - acc: 0.4842 - val_loss: 1.0172 - val_acc: 0.4848\n",
      "Epoch 167/200\n",
      " - 0s - loss: 1.0081 - acc: 0.4723 - val_loss: 0.9610 - val_acc: 0.4924\n",
      "Epoch 168/200\n",
      " - 0s - loss: 0.9965 - acc: 0.4792 - val_loss: 0.9108 - val_acc: 0.5758\n",
      "Epoch 169/200\n",
      " - 0s - loss: 0.9990 - acc: 0.4803 - val_loss: 0.8485 - val_acc: 0.6591\n",
      "Epoch 170/200\n",
      " - 0s - loss: 1.0036 - acc: 0.4787 - val_loss: 0.8993 - val_acc: 0.5833\n",
      "Epoch 171/200\n",
      " - 0s - loss: 0.9929 - acc: 0.4843 - val_loss: 0.9698 - val_acc: 0.5227\n",
      "Epoch 172/200\n",
      " - 0s - loss: 0.9920 - acc: 0.4852 - val_loss: 1.0595 - val_acc: 0.4394\n",
      "Epoch 173/200\n",
      " - 0s - loss: 1.0061 - acc: 0.4757 - val_loss: 0.9657 - val_acc: 0.4848\n",
      "Epoch 174/200\n",
      " - 0s - loss: 0.9833 - acc: 0.4981 - val_loss: 0.8883 - val_acc: 0.5985\n",
      "Epoch 175/200\n",
      " - 0s - loss: 0.9975 - acc: 0.4840 - val_loss: 0.9492 - val_acc: 0.5227\n",
      "Epoch 176/200\n",
      " - 0s - loss: 0.9923 - acc: 0.4869 - val_loss: 0.9435 - val_acc: 0.5303\n",
      "Epoch 177/200\n",
      " - 0s - loss: 0.9986 - acc: 0.4779 - val_loss: 0.9965 - val_acc: 0.4545\n",
      "Epoch 178/200\n",
      " - 0s - loss: 0.9874 - acc: 0.4880 - val_loss: 0.9278 - val_acc: 0.5379\n",
      "Epoch 179/200\n",
      " - 0s - loss: 1.0032 - acc: 0.4782 - val_loss: 0.8595 - val_acc: 0.6288\n",
      "Epoch 180/200\n",
      " - 0s - loss: 0.9888 - acc: 0.4883 - val_loss: 0.9268 - val_acc: 0.5758\n",
      "Epoch 181/200\n",
      " - 0s - loss: 0.9789 - acc: 0.5028 - val_loss: 0.7973 - val_acc: 0.6515\n",
      "Epoch 182/200\n",
      " - 0s - loss: 1.0168 - acc: 0.4776 - val_loss: 0.9145 - val_acc: 0.5682\n",
      "Epoch 183/200\n",
      " - 0s - loss: 0.9868 - acc: 0.4885 - val_loss: 1.2520 - val_acc: 0.4167\n",
      "Epoch 184/200\n",
      " - 0s - loss: 1.0054 - acc: 0.4811 - val_loss: 0.9276 - val_acc: 0.5833\n",
      "Epoch 185/200\n",
      " - 0s - loss: 0.9924 - acc: 0.4889 - val_loss: 0.8531 - val_acc: 0.6439\n",
      "Epoch 186/200\n",
      " - 1s - loss: 0.9944 - acc: 0.4852 - val_loss: 1.0054 - val_acc: 0.4545\n",
      "Epoch 187/200\n",
      " - 0s - loss: 0.9863 - acc: 0.4835 - val_loss: 0.9710 - val_acc: 0.5530\n",
      "Epoch 188/200\n",
      " - 0s - loss: 0.9941 - acc: 0.4885 - val_loss: 0.9066 - val_acc: 0.5682\n",
      "Epoch 189/200\n",
      " - 0s - loss: 0.9856 - acc: 0.4906 - val_loss: 0.9396 - val_acc: 0.5455\n",
      "Epoch 190/200\n",
      " - 0s - loss: 0.9911 - acc: 0.4858 - val_loss: 1.1028 - val_acc: 0.4318\n",
      "Epoch 191/200\n",
      " - 0s - loss: 0.9926 - acc: 0.4876 - val_loss: 0.9711 - val_acc: 0.4773\n",
      "Epoch 192/200\n",
      " - 0s - loss: 0.9897 - acc: 0.4899 - val_loss: 0.8439 - val_acc: 0.6364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200\n",
      " - 0s - loss: 0.9821 - acc: 0.4843 - val_loss: 0.9089 - val_acc: 0.5606\n",
      "Epoch 194/200\n",
      " - 0s - loss: 0.9814 - acc: 0.4993 - val_loss: 0.8016 - val_acc: 0.6667\n",
      "Epoch 195/200\n",
      " - 0s - loss: 0.9946 - acc: 0.4865 - val_loss: 0.8693 - val_acc: 0.5758\n",
      "Epoch 196/200\n",
      " - 0s - loss: 0.9894 - acc: 0.4871 - val_loss: 0.9005 - val_acc: 0.5909\n",
      "Epoch 197/200\n",
      " - 0s - loss: 0.9894 - acc: 0.4906 - val_loss: 0.8999 - val_acc: 0.5606\n",
      "Epoch 198/200\n",
      " - 0s - loss: 0.9954 - acc: 0.4821 - val_loss: 0.9055 - val_acc: 0.5606\n",
      "Epoch 199/200\n",
      " - 0s - loss: 0.9897 - acc: 0.4906 - val_loss: 0.9410 - val_acc: 0.5076\n",
      "Epoch 200/200\n",
      " - 0s - loss: 0.9871 - acc: 0.4910 - val_loss: 0.9107 - val_acc: 0.5758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c142ed490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = transformer_man([-2,-2.5,-3],\n",
    "    lambda x, c :  x == c, \n",
    "    lambda x : -2-binsize*x.index(1))\n",
    "\n",
    "youc.data['label'] = transformer.transform(youc.data['FeHround'])\n",
    "train['label'] = transformer.transform(train['FeHround'])\n",
    "for col in youc.colours:\n",
    "    mag1, mag2 = getMags(col)\n",
    "    train[conversion[mag1]+'-'+conversion[mag2]] = train[col]\n",
    "\n",
    "model = baseModel(len(colours), 3, Nlayers = 6,Nodes = 256)\n",
    "params = {\n",
    "    'x' : train[colours].values,\n",
    "    'y' : np.array([x for x in train['label'].values]),\n",
    "    'verbose' : 2,\n",
    "    'batch_size': 1000,\n",
    "    'epochs' : 200,\n",
    "    'validation_data' : \n",
    "    (youc.data[youc.data['test_flag']==1][colours].values, np.array([x for x in youc.data[youc.data['test_flag']==1]['label'].values]))\n",
    "}\n",
    "\n",
    "model.fit(**params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| [Fe/H]< | purity | completness |\n",
      "----------------------------------\n",
      "| -2.0    | 1.0000 | 1.0000      |\n",
      "| -2.5    | 0.5069 | 0.7273      |\n",
      "| -3.0    | 0.1420 | 0.5000      |\n",
      "\n",
      "          -2.0      -2.5      -3.0\n",
      "-2.0  0.555831  0.250620  0.193548\n",
      "-2.5  0.285024  0.420290  0.294686\n",
      "-3.0  0.217391  0.282609  0.500000 \n",
      "\n",
      "| [Fe/H]< | purity | completness |\n",
      "----------------------------------\n",
      "| -2.0    | 1.0000 | 1.0000      |\n",
      "| -2.5    | 0.5538 | 0.7200      |\n",
      "| -3.0    | 0.1212 | 0.5714      |\n",
      "\n",
      "          -2.0      -2.5      -3.0\n",
      "-2.0  0.646341  0.134146  0.219512\n",
      "-2.5  0.302326  0.441860  0.255814\n",
      "-3.0  0.142857  0.285714  0.571429 \n",
      "\n",
      "| [Fe/H]< | purity | completness |\n",
      "----------------------------------\n",
      "| -2.0    | 1.0000 | 1.0000      |\n",
      "| -2.5    | 0.4081 | 0.9921      |\n",
      "| -3.0    | 0.1098 | 0.6304      |\n",
      "\n",
      "          -2.0      -2.5      -3.0\n",
      "-2.0  0.096774  0.531017  0.372208\n",
      "-2.5  0.009662  0.579710  0.410628\n",
      "-3.0  0.000000  0.369565  0.630435 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretrans = lambda data : [[1 if False not in [x[i] > y or y == x[i] for y in x ] else 0 for i in range(len(x))] for x in data] \n",
    "pretrans2 = lambda data : [[0,0,1] if x[2]>0.7 else [0,1,0] if x[1]>0.33 else [1,0,0] for x in data]\n",
    "z = model.predict(youc.data[colours])\n",
    "youc.data['FeH_pred'] = transformer.inverse_transform(pretrans(z))\n",
    "            \n",
    "youc.data['FeH'] = youc.data['FeHadop']\n",
    "youc.data['FeHadop'] = youc.data['FeHround']\n",
    "youc.data['FeHround_prist'] = putbin(youc.data['FeH_pristine'])\n",
    "i = list(classes)\n",
    "\n",
    "\n",
    "data = youc.data\n",
    "pur, com = psMetric(data, inc = i)\n",
    "print printTable(i ,pur,com)\n",
    "print pd.DataFrame(conf_matrix(data, classes), columns = classes, index = classes) , '\\n'      \n",
    "             \n",
    "pur2, com2 = psMetric(data[data['test_flag']==1], inc = i)\n",
    "print printTable(i,pur2,com2)\n",
    "print pd.DataFrame(conf_matrix(data[data['test_flag']==1], classes), columns = classes, index = classes),'\\n'  \n",
    "\n",
    "\n",
    "pur3, com3 = psMetric(data, inc = i, pred = 'FeHround_prist')\n",
    "print printTable(i,pur3,com3)\n",
    "print pd.DataFrame(conf_matrix(data, classes, pred = 'FeHround_prist'), columns = classes, index = classes),'\\n'  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in classes:    \n",
    "    print len(train[train['FeHround']==c]), c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, colours, classes, binsize, data, filename):\n",
    "    try:\n",
    "        joblib\n",
    "    except NameError:\n",
    "        import joblib\n",
    "    \n",
    "    joblib.dump(model, filename+\".P\")\n",
    "    joblib.dump(\n",
    "    {\n",
    "        'colours' : colours,\n",
    "        'classes' : classes,\n",
    "        'binsize' : binsize\n",
    "    }, filename+'Params.P'\n",
    "    )\n",
    "    joblib.dump(data, filename+'Data.P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save(model, colours, classes, binsize, youc.data, 'april16v3')\n",
    "print sum([ 1 if x else 0 for x in youc.data['FeHadop']==-3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FeH_pred'] = [classes[x.index(1)] for x in pretrans(model.predict(train[colours]))]\n",
    "pur, com = psMetric(train, inc = i)\n",
    "print printTable(i ,pur,com)\n",
    "print pd.DataFrame(conf_matrix(train, classes), columns = classes, index = classes) , '\\n'  \n",
    "\n",
    "\n",
    "pur, com = psMetric(data[data['test_flag']==0], inc = i)\n",
    "print printTable(i ,pur,com)\n",
    "print pd.DataFrame(conf_matrix(data[data['test_flag']==0], classes), columns = classes, index = classes) , '\\n'  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
